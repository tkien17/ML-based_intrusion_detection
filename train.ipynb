{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecfb065c-2f31-47c2-85bc-5be1f9c3cb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\tkien\\miniconda3\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\tkien\\miniconda3\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tkien\\miniconda3\\lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: scipy in c:\\users\\tkien\\miniconda3\\lib\\site-packages (1.16.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tkien\\miniconda3\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: scikit-plot in c:\\users\\tkien\\miniconda3\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tkien\\miniconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tkien\\miniconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tkien\\miniconda3\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tkien\\miniconda3\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tkien\\miniconda3\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tkien\\miniconda3\\lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\tkien\\miniconda3\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tkien\\miniconda3\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\tkien\\miniconda3\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\tkien\\miniconda3\\lib\\site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\tkien\\miniconda3\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\tkien\\miniconda3\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tkien\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy matplotlib scipy scikit-learn scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d9db8d7-9ee9-4ee3-b91e-0aa2e018d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da7748f6-cdb0-4f52-80f2-289b0c023c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('Data/UNSW_NB15_training-set.csv')\n",
    "test_df = pd.read_csv('Data/UNSW_NB15_testing-set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4429aa5b-bcdb-4444-8fb1-524ef9a4bf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (175341, 45)\n",
      "Testing set shape: (82332, 45)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Testing set shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7592ae0a-8855-477e-bcaa-79177ba9c407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a16705f-6240-47d9-9cd5-ecfda89382fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1: drop \"id\" column\n",
    "train_df.drop(columns=[\"id\"], inplace=True, errors='ignore')\n",
    "test_df.drop(columns=[\"id\"], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5306b19e-0856-4698-8750-882bb0a30bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2: Fill null values in \"service\" col\n",
    "train_df['service'] = train_df['service'].replace('-', 'other').fillna('other')\n",
    "test_df['service'] = test_df['service'].replace('-', 'other').fillna('other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7603ea71-f3a0-45da-845c-993f722a8a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175341, 196)\n",
      "(82332, 196)\n"
     ]
    }
   ],
   "source": [
    "#1.3: One-hot encoding proto, service, state\n",
    "nominal_features = ['proto', 'service', 'state']\n",
    "train_df = pd.get_dummies(train_df, columns=nominal_features)\n",
    "test_df = pd.get_dummies(test_df, columns=nominal_features)\n",
    "test_df = test_df.reindex(columns=train_df.columns, fill_value=0)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea628393-b05a-42da-adf4-235e62fd96a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175341, 194)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "y_train_binary = train_df['label']\n",
    "y_test_binary = test_df['label']\n",
    "\n",
    "y_train_multiclass = train_df['attack_cat']\n",
    "y_test_multiclass = test_df['attack_cat']\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_multiclass = le.fit_transform(y_train_multiclass)\n",
    "y_test_multiclass = le.transform(y_test_multiclass)\n",
    "\n",
    "X_train_base = train_df.drop(columns = ['label', 'attack_cat'])\n",
    "X_test_base = test_df.drop(columns = ['label', 'attack_cat'])\n",
    "\n",
    "\n",
    "print(X_train_base.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9369cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Feature reduction\n",
    "\n",
    "k=4 #number of features to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55c4e1ad-8702-4c60-9025-d521a98581a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dpkts', 'spkts', 'dbytes', 'dloss']\n",
      "        dpkts  spkts  dbytes  dloss\n",
      "0           4      6     172      0\n",
      "1          38     14   42014     17\n",
      "2          16      8   13186      6\n",
      "3          12     12     770      3\n",
      "4           6     10     268      1\n",
      "...       ...    ...     ...    ...\n",
      "175336      0      2       0      0\n",
      "175337      8     10     354      1\n",
      "175338      0      2       0      0\n",
      "175339      0      2       0      0\n",
      "175340      0      2       0      0\n",
      "\n",
      "[175341 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# 2.1: Feature Selection (correlation matrix) \n",
    "start_select_time_train = time.time()                  #to calculate timeFR_train\n",
    "correlation_matrix = X_train_base.corr()\n",
    "C_i = correlation_matrix.mean()\n",
    "selected_features = C_i.sort_values(ascending=False).head(k).index.tolist()\n",
    "X_train_sel = X_train_base[selected_features]\n",
    "FS_train_time = time.time() - start_select_time_train  #feature selection train time\n",
    "\n",
    "start_select_time_test = time.time()                   #to calculate timeFR_test\n",
    "X_test_sel = X_test_base[selected_features]\n",
    "FS_test_time = time.time() - start_select_time_test    #feature selection time\n",
    "\n",
    "print(selected_features)\n",
    "print(X_train_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b2d37d0-4eb6-4d1c-9754-e949cc61d628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.38882863  0.28883011  0.40652493 -0.39708978]\n",
      " [ 1.5398475   0.08039593 -0.05993712 -0.41949462]\n",
      " [ 1.59565277  0.13570403 -0.08215488 -0.35166054]\n",
      " ...\n",
      " [-1.44790597 -0.51825894  0.09849325  0.06423195]\n",
      " [-1.632304   -0.80224102  0.34373997 -0.20259839]\n",
      " [-1.63504559 -0.80642334  0.34753581 -0.20509896]]\n"
     ]
    }
   ],
   "source": [
    "# 2.2: Feature Extraction (PCA) \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "pca= PCA(n_components=k)\n",
    "\n",
    "start_extract_time_train = time.time(); #to calculate timeFR_train\n",
    "X_train_scaled = scaler.fit_transform(X_train_base)\n",
    "X_train_ext = pca.fit_transform(X_train_scaled)\n",
    "FE_train_time = time.time() - start_extract_time_train \n",
    "\n",
    "start_extract_time_test = time.time(); #to calculate timeFR_test\n",
    "X_test_scaled = scaler.transform(X_test_base) \n",
    "X_test_ext = pca.transform(X_test_scaled)\n",
    "FE_test_time = time.time() - start_extract_time_test \n",
    "\n",
    "print(X_train_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22b45bd5-8100-491a-b027-ae115ad4fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3: Attact classifiers  \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"RandomForest\": RandomForestClassifier(max_depth=5),\n",
    "    \"KNeighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"MLP\": MLPClassifier(max_iter=100, hidden_layer_sizes=200),\n",
    "    \"Naive Bayes\": BernoulliNB()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b9d0562-f8b1-43a4-a6ad-7809b835acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4: Evaluate metrics \n",
    "\n",
    "def get_metrics(model, X_train, y_train, X_test, y_test, FR_train_time, FR_test_time):\n",
    "    \n",
    "    #training time = fit time + feature reduction train time\n",
    "    start_train=time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    total_train_time= time.time() - start_train + FR_train_time\n",
    "\n",
    "    #inference time = predict time + feature reduction test time\n",
    "    start_test = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    total_infer_time = time.time() - start_test + FR_test_time\n",
    "    #convert to Microseconds per Sample\n",
    "    n_samples = len(y_test)\n",
    "    inference_us_per_sample = (total_infer_time / n_samples) * 1_000_000 \n",
    "    \n",
    "    #precision, recall, f1-score\n",
    "    p = precision_score(y_test, y_pred, average='weighted') * 100\n",
    "    r = recall_score(y_test, y_pred, average='weighted') * 100\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted') * 100\n",
    "\n",
    "    return [p, r, f1, total_train_time, inference_us_per_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c3a8270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Decision Tree...\n",
      "Evaluating RandomForest...\n",
      "Evaluating KNeighbors...\n",
      "Evaluating MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tkien\\miniconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Naive Bayes...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">Feature Extraction</th>\n",
       "      <th colspan=\"5\" halign=\"left\">Feature Selection</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>F1</th>\n",
       "      <th>Training (s)</th>\n",
       "      <th>Inference (us)</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>F1</th>\n",
       "      <th>Training (s)</th>\n",
       "      <th>Inference (us)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>85.76</td>\n",
       "      <td>84.67</td>\n",
       "      <td>84.37</td>\n",
       "      <td>2.67</td>\n",
       "      <td>5.24</td>\n",
       "      <td>84.09</td>\n",
       "      <td>79.89</td>\n",
       "      <td>78.75</td>\n",
       "      <td>17.28</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>85.75</td>\n",
       "      <td>81.35</td>\n",
       "      <td>80.31</td>\n",
       "      <td>20.39</td>\n",
       "      <td>9.50</td>\n",
       "      <td>79.14</td>\n",
       "      <td>75.88</td>\n",
       "      <td>74.52</td>\n",
       "      <td>22.15</td>\n",
       "      <td>5.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighbors</th>\n",
       "      <td>86.24</td>\n",
       "      <td>84.70</td>\n",
       "      <td>84.32</td>\n",
       "      <td>1.77</td>\n",
       "      <td>11.08</td>\n",
       "      <td>52.56</td>\n",
       "      <td>48.06</td>\n",
       "      <td>42.89</td>\n",
       "      <td>17.35</td>\n",
       "      <td>1190.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP</th>\n",
       "      <td>85.81</td>\n",
       "      <td>81.87</td>\n",
       "      <td>80.95</td>\n",
       "      <td>466.41</td>\n",
       "      <td>6.18</td>\n",
       "      <td>76.79</td>\n",
       "      <td>75.30</td>\n",
       "      <td>74.42</td>\n",
       "      <td>131.65</td>\n",
       "      <td>2.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>72.55</td>\n",
       "      <td>71.90</td>\n",
       "      <td>71.12</td>\n",
       "      <td>1.58</td>\n",
       "      <td>5.21</td>\n",
       "      <td>75.48</td>\n",
       "      <td>73.63</td>\n",
       "      <td>73.59</td>\n",
       "      <td>17.19</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Feature Extraction                                            \\\n",
       "                               P      R     F1 Training (s) Inference (us)   \n",
       "Decision Tree              85.76  84.67  84.37         2.67           5.24   \n",
       "RandomForest               85.75  81.35  80.31        20.39           9.50   \n",
       "KNeighbors                 86.24  84.70  84.32         1.77          11.08   \n",
       "MLP                        85.81  81.87  80.95       466.41           6.18   \n",
       "Naive Bayes                72.55  71.90  71.12         1.58           5.21   \n",
       "\n",
       "              Feature Selection                                            \n",
       "                              P      R     F1 Training (s) Inference (us)  \n",
       "Decision Tree             84.09  79.89  78.75        17.28           0.12  \n",
       "RandomForest              79.14  75.88  74.52        22.15           5.02  \n",
       "KNeighbors                52.56  48.06  42.89        17.35        1190.14  \n",
       "MLP                       76.79  75.30  74.42       131.65           2.19  \n",
       "Naive Bayes               75.48  73.63  73.59        17.19           0.20  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 selected/extracted features and binary classification results\n"
     ]
    }
   ],
   "source": [
    "#5.1: Display binary classification results\n",
    "results_data = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    \n",
    "    # Feature Extraction results \n",
    "    from sklearn.base import clone\n",
    "    model_ext = clone(model)\n",
    "    ext_metrics = get_metrics(model_ext, X_train_ext, y_train_binary, X_test_ext, y_test_binary, FE_train_time, FE_test_time)\n",
    "    \n",
    "    # Feature Selection results \n",
    "    model_sel = clone(model)\n",
    "    sel_metrics = get_metrics(model_sel, X_train_sel, y_train_binary, X_test_sel, y_test_binary, FS_train_time, FS_test_time)\n",
    "    \n",
    "    row = [name] + ext_metrics + sel_metrics\n",
    "    results_data.append(row)\n",
    "\n",
    "# Format dataframe\n",
    "columns = pd.MultiIndex.from_product(\n",
    "    [[\"Feature Extraction\", \"Feature Selection\"], \n",
    "     [\"P\", \"R\", \"F1\", \"Training (s)\", \"Inference (us)\"]]\n",
    ")\n",
    "\n",
    "df_results = pd.DataFrame(\n",
    "    [r[1:] for r in results_data], \n",
    "    index=[r[0] for r in results_data], \n",
    "    columns=columns\n",
    ")\n",
    "\n",
    "display(df_results.round(2))\n",
    "print(\"4 selected/extracted features and binary classification results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec89b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Decision Tree...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tkien\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RandomForest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tkien\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\tkien\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating KNeighbors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tkien\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MLP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tkien\\miniconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tkien\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\tkien\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\tkien\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Naive Bayes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tkien\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">Feature Extraction</th>\n",
       "      <th colspan=\"5\" halign=\"left\">Feature Selection</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>F1</th>\n",
       "      <th>Training (s)</th>\n",
       "      <th>Inference (us)</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>F1</th>\n",
       "      <th>Training (s)</th>\n",
       "      <th>Inference (us)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>76.16</td>\n",
       "      <td>67.39</td>\n",
       "      <td>70.97</td>\n",
       "      <td>3.10</td>\n",
       "      <td>5.33</td>\n",
       "      <td>69.48</td>\n",
       "      <td>61.26</td>\n",
       "      <td>61.18</td>\n",
       "      <td>17.33</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>77.64</td>\n",
       "      <td>64.18</td>\n",
       "      <td>65.78</td>\n",
       "      <td>24.84</td>\n",
       "      <td>13.24</td>\n",
       "      <td>55.78</td>\n",
       "      <td>58.85</td>\n",
       "      <td>55.43</td>\n",
       "      <td>22.68</td>\n",
       "      <td>7.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighbors</th>\n",
       "      <td>77.93</td>\n",
       "      <td>69.41</td>\n",
       "      <td>72.64</td>\n",
       "      <td>1.77</td>\n",
       "      <td>11.07</td>\n",
       "      <td>49.79</td>\n",
       "      <td>45.48</td>\n",
       "      <td>35.47</td>\n",
       "      <td>17.34</td>\n",
       "      <td>447.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP</th>\n",
       "      <td>78.81</td>\n",
       "      <td>68.58</td>\n",
       "      <td>69.63</td>\n",
       "      <td>1071.54</td>\n",
       "      <td>6.64</td>\n",
       "      <td>58.18</td>\n",
       "      <td>62.44</td>\n",
       "      <td>54.93</td>\n",
       "      <td>233.96</td>\n",
       "      <td>6.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>62.75</td>\n",
       "      <td>50.80</td>\n",
       "      <td>53.74</td>\n",
       "      <td>1.65</td>\n",
       "      <td>5.38</td>\n",
       "      <td>41.55</td>\n",
       "      <td>59.68</td>\n",
       "      <td>48.54</td>\n",
       "      <td>17.40</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Feature Extraction                                            \\\n",
       "                               P      R     F1 Training (s) Inference (us)   \n",
       "Decision Tree              76.16  67.39  70.97         3.10           5.33   \n",
       "RandomForest               77.64  64.18  65.78        24.84          13.24   \n",
       "KNeighbors                 77.93  69.41  72.64         1.77          11.07   \n",
       "MLP                        78.81  68.58  69.63      1071.54           6.64   \n",
       "Naive Bayes                62.75  50.80  53.74         1.65           5.38   \n",
       "\n",
       "              Feature Selection                                            \n",
       "                              P      R     F1 Training (s) Inference (us)  \n",
       "Decision Tree             69.48  61.26  61.18        17.33           0.22  \n",
       "RandomForest              55.78  58.85  55.43        22.68           7.60  \n",
       "KNeighbors                49.79  45.48  35.47        17.34         447.90  \n",
       "MLP                       58.18  62.44  54.93       233.96           6.24  \n",
       "Naive Bayes               41.55  59.68  48.54        17.40           0.44  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 selected/extracted features and multi-class classification results\n"
     ]
    }
   ],
   "source": [
    "#5.2: Display multi-class classification results\n",
    "results_data_1 = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    \n",
    "    # Feature Extraction results \n",
    "    from sklearn.base import clone\n",
    "    model_ext = clone(model)\n",
    "    ext_metrics = get_metrics(model_ext, X_train_ext, y_train_multiclass, X_test_ext, y_test_multiclass, FE_train_time, FE_test_time)\n",
    "    \n",
    "    # Feature Selection results \n",
    "    model_sel = clone(model)\n",
    "    sel_metrics = get_metrics(model_sel, X_train_sel, y_train_multiclass, X_test_sel, y_test_multiclass, FS_train_time, FS_test_time)\n",
    "    \n",
    "    row = [name] + ext_metrics + sel_metrics\n",
    "    results_data_1.append(row)\n",
    "\n",
    "# Format dataframe\n",
    "columns = pd.MultiIndex.from_product(\n",
    "    [[\"Feature Extraction\", \"Feature Selection\"], \n",
    "     [\"P\", \"R\", \"F1\", \"Training (s)\", \"Inference (us)\"]]\n",
    ")\n",
    "\n",
    "df_results = pd.DataFrame(\n",
    "    [r[1:] for r in results_data_1], \n",
    "    index=[r[0] for r in results_data_1], \n",
    "    columns=columns\n",
    ")\n",
    "\n",
    "display(df_results.round(2))\n",
    "print(\"4 selected/extracted features and multi-class classification results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
